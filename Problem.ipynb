{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrJzggfH2YEG"
      },
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "<a name=\"MATH\"></a>\n",
        "## E) Memory Efficient Backprop [Difficulty: Medium to Hard] [Max points: 10]\n",
        "\n",
        "In LLMs, the last layer is a projection matrix to calculate the probabilities of the next token, ie $\\sigma(XW)$. However, if the vocabulary size is very large, say 128K, then the materialization of the logits causes VRAM spikes.\n",
        "\n",
        "For example, if the `bsz = 4, qlen = 4096, hd = 4096, vocab = 128K`, then the memory usage for the logits in bfloat16 would be 4GB. In the worst case, we might even need to upcast logits to float32, so 8GB is needed.\n",
        "\n",
        "In Unsloth, we utilize [Apple's Cut Cross Entropy Loss](https://machinelearning.apple.com/research/cut-your-losses) to reduce VRAM usage, by allowing a Triton kernel to create the logits on the fly to calculate the cross entropy loss. But this does not generalize well to other functions.\n",
        "\n",
        "Our goal is to generalize this ultimately, but directly creating logits on the fly will be hard. Instead, let's take a slightly less complex approach. Let's first review some stuff. We first notice that during the normal case after forming the intermediate logits for 2 batches, we then do a gather function to aggregate the intermediate results into a single column:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} \\times W &= \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\\\\n",
        "f \\bigg( \\begin{bmatrix} x_1 W \\\\ x_2 W \\end{bmatrix} \\bigg) &= \\begin{pmatrix} y_1 \\\\ y_2 \\end{pmatrix}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "So, if we can somehow skip the materialization of the intermediate logits, and just output the output of `f`, we can save a lot of VRAM!\n",
        "\n",
        "Notice during backpropagation we can use the chain rule:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{dX} &= \\frac{dL}{dy} \\frac{dy}{dX} ; \\frac{dL}{dW} = \\frac{dL}{dy} \\frac{dy}{dW} \\\\\n",
        "\\frac{dL}{dy} &= \\text{Downstream from backprop} \\\\\n",
        "\\frac{dy}{dX} &= W^T \\\\\n",
        "\\frac{dy}{dW} &= X^T \\\\\n",
        "\\frac{dL}{dX} &= \\frac{dL}{dy} W^T \\\\\n",
        "\\frac{dL}{dW} &= X^T \\frac{dL}{dy} \\\\\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "If we simply compute the intermediate tensors on the fly via batches, say we do batch 1, then batch 2, we can reduce VRAM usage from 4GB to 2GB!\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\frac{dL}{dX} &= \\begin{bmatrix} \\frac{dL_1}{dy_1} W^T \\\\ \\frac{dL_2}{dy_2} W^T \\end{bmatrix} \\\\\n",
        "\\frac{dL}{dW} &= \\bigg( X_1^T \\frac{dL_1}{dy_1} + X_2^T  \\frac{dL_2}{dy_2} \\bigg)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "1. Your goal is to write a `torch.autograd.Function` with a `forward` and `backward` pass showcasing this memory efficient implementation.\n",
        "\n",
        "2. You must NOT hard code the derivatives - move the transformation function from the logits / intermeditate tensors to a smaller tensor as a separate function which can allow `autograd` to pass through it.\n",
        "\n",
        "3. As a hint, look at `torch.checkpoint` at https://github.com/pytorch/pytorch/blob/main/torch/utils/checkpoint.py. Also, don't forget about the upstream gradients! We need to multiply them to the current gradients!\n",
        "\n",
        "4. Make the Cross Entropy Loss work. You must show other functions working as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rp-IJIbv90f6"
      },
      "outputs": [],
      "source": [
        "def transformation_function(batch, linear, labels):\n",
        "    x = linear(batch).float() # Up projection to large space\n",
        "    from torch.nn import CrossEntropyLoss\n",
        "    down_projection_function = CrossEntropyLoss(reduction = \"mean\")\n",
        "    # Down projection to small space\n",
        "    loss = down_projection_function(x.view(-1, x.shape[-1]), labels.view(-1))\n",
        "    return loss\n",
        "\n",
        "class MemoryEfficientLinear(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, X, linear, labels, forward_function):\n",
        "        outputs = []\n",
        "        # EDIT THIS FUNCTION\n",
        "        output = forward_function(X, linear, labels)\n",
        "        ctx.save_for_backward(X)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dY):\n",
        "        X = ctx.saved_tensors\n",
        "        # EDIT THIS FUNCTION\n",
        "        return X, None, None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lIczyn8-2-o"
      },
      "source": [
        "To test your implementation, it should not OOM for large inputs. Also, check the gradient is actually equivalent via `torch.allclose` in the normal approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVZ414R2Dk8M"
      },
      "source": [
        "## Marking Criteria for E) Max points = 10\n",
        "```python\n",
        "if attemped_E:\n",
        "    E_score = 0\n",
        "    if VRAM_50_percent_reduction: E_score += 2\n",
        "    if remove_float32_upcast: E_score = 0\n",
        "    if show_ce_loss_works: E_score += 1\n",
        "    if show_other_functions_work: E_score += 1\n",
        "    if hardcoded_gradients: E_score = 0\n",
        "    if allows_dynamic_chunk_sizes: E_score += 1\n",
        "    if llama_1B_training_loss_matches: E_score += 1\n",
        "    else: E_score = 0\n",
        "    if GRPO_memory_efficient_linear_works: E_score += 4\n",
        "    final_score += E_score\n",
        "else:\n",
        "    final_score += 0\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}